#!/usr/bin/env python
# coding: utf-8

# In[20]:


import os
import requests
from zipfile import ZipFile
from urllib.request import urlretrieve
import pandas as pd
import numpy as np
#import altair as alt
#from vega_datasets import data
get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns
sns.set()
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import statsmodels.api as sm
from sklearn.feature_selection import RFECV
from sklearn.ensemble import *
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import *
from sklearn.metrics import *
import pycaret
from pycaret.classification import *
from imblearn.over_sampling import SMOTE
from sklearn.inspection import permutation_importance
import re
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV


# In[15]:


df = pd.read_csv(r'C:\Users\tegan\Downloads\clean_df.csv')
df.rename(columns={'Ethnicity_Multiracial, non-Hispanic': 'Ethnicity_Multiracial non-Hispanic'}, inplace=True)
df.head()


# In[21]:


# Separate predictors and target
X, y = df.drop('Cancer', axis=1), df.Cancer

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale data
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)


# ### Feature Importance from RandomForestClassifier

# In[22]:


forest = RandomForestClassifier(n_estimators=100,
                                max_depth=10,
                                random_state=42)

forest.fit(X_train_res, y_train_res)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]


# In[23]:


plt.ylabel('Feature importance')
plt.bar(range(X_train.shape[1]), 
        importances[indices],
        align='center')

feat_labels = X.columns
plt.xticks(range(X_train.shape[1]), 
           feat_labels[indices], rotation=90)

plt.xlim([-1, 15])

plt.tight_layout()
plt.show()


# ### Compare models using imbalanced data

# In[7]:


clf = setup(df, target='Cancer', session_id=123)

best_model = compare_models()


# ### Compare models using SMOTE data

# In[10]:


clf = setup(df, target='Cancer', fix_imbalance=True, session_id=123)

best_model = compare_models()


# ### Naive Bayes using SMOTE data

# In[12]:


nb_model = create_model('nb')


# In[26]:


print(nb_model)


# In[19]:


y_pred = nb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')


# ### Naive Bayes grid search

# In[24]:


param_grid_nb = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
}

nb_model_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)
nb_model_grid.fit(X_train_res, y_train_res)
print(nb_model_grid.best_estimator_)


# In[25]:


y_pred = nb_model_grid.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')


# In[ ]:




